# Team-Info
| (1) 과제명 | 외국인 한국어 학습자를 위한 음성 인식 기반 한국어 발음 교정 및 회화 연습 서비스
|:---  |---  |
| (2) 팀 번호 / 팀 이름 | 26-Joggim' |
| (3) 팀 구성원 | 조하은 (2271058): 리더, 백엔드 개발 <br> 김민지 (2271013): 팀원, 프론트엔드 개발, AI 개발 <br> 김서윤 (2271014) : 팀원, AI 개발			 |
| (4) 팀 지도교수 | 심재형 교수님 |
| (5) 과제 분류 | 산학 과제 |
| (6) 과제 키워드 | 음성 인식, STT, 추천 시스템, 교육  |
| (7) 과제 내용 요약 | K-Talk은 외국인을 위한 한국어 발음 교정 서비스로, 음성 인식 기술을 활용해 사용자가 문장을 발음하고 정교한 피드백을 받을 수 있도록 지원한다. 사용자의 음성 입력을 분석하여 음소 단위의 발음 오류를 진단하고, 발음 오류 유형 태그 기반의 개인 맞춤 학습 경로를 제공함으로써 발음 정확도 향상을 효과적으로 돕는다. 이 서비스는 STT, TTS를 포함한 다양한 음성 기술을 바탕으로 보다 효율적인 한국어 학습을 지원한다. |

<br>

# Project-Summary
| 항목 | 내용 |
|:---  |---  |
| (1) 문제 정의 | **1. 배경**<br> • 한국에 거주하는 외국인은 2025년 기준 265만 명 이상이며, 이들 중 많은 수가 한국어 발음 문제로 인해 실질적인 의사소통의 어려움을 겪고 있음. <br> • 한국어의 연음·된소리·받침 발음 등은 비원어민에게 익숙하지 않아, 발음 오류가 의미 전달 실패, 소통 단절, 자신감 저하로 이어짐. <br> • 현재 한국어 교육은 문법·어휘 중심으로 구성되어 있어, 실시간 피드백을 통한 발음 교정이 어려움.  <br> <br> **2. Target Customer** <br> •	한국어 발음을 정확하게 익히고자 하는 외국인 학습자	<br> •	실생활에서 명확한 의사소통이 요구되는 외국인 직장인 및 유학생 등 <br> <br> **3. Pain Points**	<br> •	발음 오류로 인한 의미 전달 실패	<br> •	소통 불편 → 사회·직장 내 적응 어려움 <br> •	현재 서비스는 발음 피드백이나 교정 기능이 부족함	<br> •	어떤 발음을 틀렸는지 스스로 알기 어려움|
| (2) 기존연구와의 비교 |**1. KoKoA 코코아 : 한국어 학습 프로그램**<br>  장점:  한국어 시험(Topik) 대비  가능 , 다양한 퀴즈 학습 제공<br>  단점: 실시간 피드백 없음, 맞춤형 학습 경로 제공 미흡<br><br>**2. Marine 미리내 : AI 한국어 학습**<br>  장점: 관용구 및 신조어 공부 기능, 문장을 AI가 분석하여 문법, 단어, 어순 등을 상세히 설명<br>  단점: 발음 교정 기능이 없어 말하기 학습에 적합하지 않음<br><br>	**3.	GANADA (가나다)** : 실생활 기반 한국어 말하기 훈련 플랫폼<br>장점: 상황별 회화 연습 제공, 사용자 음성 녹음 가능<br>단점: 정밀한 발음 분석 부족, 문법 피드백 및 개인화 기능 미흡 <br><br> → 본 프로젝트의 차별성(K-Talk) : 단순히 TTS 발음을 들려주는 것이 아니라, wav2vec2 기반 발음 분석 + 피드백 + 문장 구조 교정 + 개인 맞춤 학습 루트 추천까지 모두 제공하는 종합 솔루션|
| (3) 제안 내용 | 본 프로젝트는 AI 기반 발음 교정 및 대화형 한국어 학습 플랫폼을 제안함 <br><br>음성 인식 모델과 TTS, GPT를 활용하여 다음과 같은 기능을 제공:<br>• 사용자의 발음을 실시간으로 분석하고 발음 오류를 시각적으로 피드백<br>• TTS 기반 모범 발음을 제공하여 비교 학습 가능<br>• GPT 기반 문법 오류 피드백과 자연스러운 문장 추천<br>• 사용자의 발음 패턴을 분석해 개선이 필요한 음소·문장을 선별하고 추천<br><br>→ 단순한 어휘 암기가 아닌, 실제 커뮤니케이션 능력 향상에 중점을 둠.|
| (4) 기대효과 및 의의 | •	정확한 한국어 발음 습득을 통해 외국인의 사회 적응력 강화 <br>	•	실생활·직장 내 커뮤니케이션 능력 향상 <br>	•	개별 발음 패턴을 분석한 맞춤형 학습 제공으로 학습 효율 극대화 <br>	•	AI 기반 발음 교정 기술을 활용한 차세대 한국어 학습 서비스 모델 제시 |
| (5) 주요 기능 리스트 | **1. 발음 분석 및 피드백** <br>	음성 인식 모델(wav2vec2)을을 활용하여 발음을 텍스트로 변환하고, 모범 발음과 비교하여 오류를 시각적으로 피드백 <br><br>**2. 음성합성(TTS)**<br>	 Google TTS를 활용해 학습자가 정확한 발음을 들으며 따라할 수 있도록 모범 발음 제공 <br><br> **3. GPT 기반 문장 교정 및 대화 연습**<br>	 실전 대화 연습을 통해 문법, 표현 오류를 잡아주고 자연스러운 표현을 학습하도록 지원 <br><br> 4. **개인 맞춤형 학습 경로 추천**<br>	 사용자의 발음 패턴을 분석하여 개선이 필요한 음소/문장을 기반으로 학습 문장 추천 |

<br>
 
# Project-Design & Implementation
| 항목 | 내용 |
|:---  |---  |
| (1) 요구사항 정의 | **1. IA** <br> ![Image](https://github.com/user-attachments/assets/61e899d6-461e-4763-8e7c-0027261d4deb) <br><br> **2. UI 분석/설계 모델** <br>![Image](https://github.com/user-attachments/assets/d7409bde-21b3-4201-b13a-ef6e97afd3fe) <br><br> **3. ER 다이어그램**<br> ![ER 다이어그램](https://github.com/user-attachments/assets/823138d9-9707-417c-92d6-ef1407cb22b1)<br>- **`User`:** 사용자 정보 저장<br>- **`Topic`:** 사용자가 학습할 주제 저장<br>- **`Sentence`:** 사용자가 학습할 문장을 저장<br>- **`LearningHistory`:** 사용자가 학습한 문장의 통과 여부, 오류 내역을 저장<br>- **`ChatRoom`:** 사용자와 챗봇 간의 채팅방을 저장<br>- **`UserMessage`:** 사용자가 챗봇에게 보낸 메시지를 저장<br>- **`BotMessage`:** 챗봇이 응답한 메시지를 저장<br>- **`GrammarFeedback`:** UserMessage에 문법 오류가 있다면 문법 교정 피드백 제공<br>- **`PronunciationFeedback`:** UserMessage에 발음 오류가 있다면 발음 교정 피드백 제공<br>- **`TokenBlacklist`:** 로그아웃된 JWT 토큰을 저장하여 인증을 차단|
| (2) 전체 시스템 구성 | ![시스템 구조도](https://github.com/user-attachments/assets/3772fef8-650d-4da1-bd9a-42fa22dcf68e)<br>**프론트엔드**<br>- React: 사용자 인터페이스 제공<br>- 사용자의 발음 녹음 및 서버 전송<br><br>**백엔드**<br>- Spring Boot: 사용자 관리, 학습 데이터 제공, AI 서버와의 API 통합<br>- AWS EC2 + Docker: 서버 애플리케이션 배포 및 컨테이너 기반 운영<br><br>**데이터베이스 및 저장소**<br>- MySQL: 사용자 정보, 학습 문장, 발음 피드백 데이터 저장<br>- AWS RDS: MySQL 데이터베이스의 안정적 클라우드 관리<br>- AWS S3: TTS로 생성된 음성 파일 저장 및 URL 제공<br><br>**AI 서버**<br>- FastAPI: STT, TTS, 피드백 기능을 제공하는 AI 서버 프레임워크<br>- wav2vec2: 사용자의 음성을 텍스트로 변환 (STT)<br>- Google TTS: 학습 문장에 대한 한국어 표준 발음 생성<br>- OpenAI API: 발음 및 문장에 대한 GPT 기반 피드백 생성 |
| (3) 주요엔진 및 기능 설계 | **1. 클라이언트: `React`**<br>- 대화 연습 화면, 음성 녹음 및 학습 화면 등 UI를 제공하여 학습자와 시스템 간의 상호작용<br>- 음성 녹음 기능을 제공하여 사용자가 발음을 입력하고 이를 서버로 전송<br>- RESTful API를 통해 백엔드와 실시간으로 통신하여 사용자 데이터를 관리하고, 학습 피드백을 실시간으로 제공<br><br>**2. 서버: `Spring Boot`**<br>- 사용자 관리, 학습 데이터 제공, 채팅 관리 등<br>- AI 모듈과 연결하여  발음 분석 및 교정, 음성 합성 요청 등을 처리<br>- AWS EC2 인스턴스를 이용하여 Docker를 통해 환경을 컨테이너화하여 배포 및 관리<br><br>**3. AI 모듈**<br>- **`FastAPI`**: 백엔드 서버 와 연결되어 API 요청을 처리하고, 각 AI 엔진에 맞는 입력값을 전달하여 처리된 결과를 다시 백엔드로 반환<br>- **`wav2vec2`**: wav2vec2는 음성 인식 모델로, 사용자의 음성을 텍스트로 변환. 텍스트로 변환된 음성은 원본 문장과 비교하여 피드백 제공에 사용<br>- **`Google TTS`**: 텍스트를 음성으로 변환하여 학습자가 문장을 정확하게 발음할 수 있도록 모범 발음을 제공<br>- **`GPT`**: 사용자가 입력한 문장에 대해 문법 오류를 수정하거나 표현을 개선하는 피드백을 제공<br><br>**4. 데이터베이스**<br>- **`MySQL`**: 사용자 정보, 학습 기록, 문장 데이터 등을 저장<br>- **`AWS RDS`**: 백업, 자동 확장성, 보안 등의 기능을 제공하여 데이터의 안정성과 관리 효율성을 높임<br>- **`AWS S3`**: 음성 데이터를 저장하고 관리<br><br>**5. 음성 인식 엔진**<br>- 사용자의 발음을 텍스트로 변환<br>- 변환된 텍스트와 원본 문장을 비교하여 오류 분석<br>- 구현 방식:<br>    - React에서 녹음된 음성을 파일로 변환<br>    - 백엔드를 통해 AI 서버로 전송 <br>    - AI 서버에서 wav2vec2 모델 활용하여 STT 변환<br>    - 변환된 텍스트를 분석 후 피드백 반환<br><br>**6. 음성 합성 엔진**<br>- 사용자가 정확한 발음을 들으며 연습할 수 있도록 음성 제공<br>- 사용자가 특정 문장을 요청하면 FastAPI 서버에서 Google TTS를 호출하여 음성 파일 생성<br>- 생성된 음성을 백엔드로 전송 후 프론트엔드에서 재생<br><br>**7. 챗봇 엔진**<br>- GPT 기반의 챗봇을 구현하여 사용자의 문법 및 표현 오류를 실시간으로 피드백<br>- 사용자가 말한 문장을 분석하여 문법 및 표현 오류 수정<br>- 보다 자연스러운 표현을 제공하고 대화를 제공하여 사용자에게 한국어 회화 연습 제공<br>- 구현 방식:<br>    - STT 변환된 텍스트를 GPT 모델에 전달하여 문장 교정 요청<br>    - 수정된 문장 및 피드백을 사용자에게 제공 |
| (4) 주요 기능의 구현 | **1. 발음 분석 및 피드백**<br>해당 기능은 STT 모델과 G2P 라이브러리를 활용하고, 그 위에 자체 구현한 음소 매핑 및 비교 로직을 통해 발음 오류를 정밀하게 평가하도록 구현하였다.<br><br>&nbsp;&nbsp;동작 흐름:<br>&nbsp;&nbsp;&nbsp;&nbsp;a. Client(학습자)가 주어진 학습 문장을 보고 직접 발음<br>&nbsp;&nbsp;&nbsp;&nbsp;b. 녹음된 음성 파일이 실시간으로 서버에 전송<br>&nbsp;&nbsp;&nbsp;&nbsp;c. 서버에서 Wav2Vec2 모델을 사용하여 음성을 텍스트로 변환<br>&nbsp;&nbsp;&nbsp;&nbsp;d. 기준 문장과 STT 결과를 G2P 및 음소 매핑을 통해 비교하고, 발음 오류를 분석<br>&nbsp;&nbsp;&nbsp;&nbsp;e. 분석된 발음 오류 정보를 클라이언트에 실시간으로 전달<br>&nbsp;&nbsp;&nbsp;&nbsp;f. 학습자는 피드백을 바탕으로 다시 발음하며, a~e 과정을 반복 수행<br><br>&nbsp;&nbsp;● **STT 모듈**<br>&nbsp;&nbsp;&nbsp;&nbsp;- 기능: 사용자의 음성 입력을 텍스트로 변환<br>&nbsp;&nbsp;&nbsp;&nbsp;- 구현: Wav2Vec2 모델(`kresnik/wav2vec2-large-xlsr-korean`)을 활용하여 업로드된 음성을 16kHz, mono wav로 전처리한 뒤 텍스트로 디코딩<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`transcribe_audio_file_wav2vec()` 함수에서 `transformers`의 Processor/Model로 로짓 예측 및 `batch_decode()` 처리<br><br>&nbsp;&nbsp;● **음소 매핑 처리 모듈**<br>&nbsp;&nbsp;&nbsp;&nbsp;- 기능: 텍스트를 음소(자모 단위)로 변환하고 각 음소의 원글자 및 위치 정보 매핑<br>&nbsp;&nbsp;&nbsp;&nbsp;- 구현: `g2pk`를 통해 발음 기반 텍스트로 변환 후, `convert_to_phonemes_with_mapping()` 함수에서 `hgtk`를 사용해 초/중/종성으로 분해<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;각 음소에 대해 글자 인덱스(mapping), 자모 유형(types), 원문 글자(chars)를 함께 저장하여 이후 분석 단계에서 활용<br><br>&nbsp;&nbsp;● **허용 발음 변형 모듈**<br>&nbsp;&nbsp;&nbsp;&nbsp;- 기능: 자연스러운 발화에서 발생할 수 있는 음운 현상을 허용하여 오류로 간주하지 않도록 처리<br>&nbsp;&nbsp;&nbsp;&nbsp;- 구현: `apply_phonological_variants()` 함수에서 정의된 `PHONOLOGICAL_RULES`를 기반으로 가능한 대체 조합을 생성<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;기준 음소 시퀀스를 변형 가능한 형태로 조합해 set으로 저장하고, 사용자 음소가 이 set에 포함될 경우 오류에서 제외<br><br>&nbsp;&nbsp;● **발음 평가 모듈**<br>&nbsp;&nbsp;&nbsp;&nbsp;- 기능: 기준 문장과 사용자 발화의 음소 차이 비교 및 오류 식별<br>&nbsp;&nbsp;&nbsp;&nbsp;- 구현: `evaluate_pronunciation_with_index()` 함수에서 음소 변환 후 `diff_by_type()`로 비교 수행<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`SequenceMatcher`를 통해 삽입/삭제/대체 유형 분류<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;공백, 문장 부호, 허용 음운 변형은 분석에서 제외<br><br>&nbsp;&nbsp;● **음소 차이 분석 모듈**<br>&nbsp;&nbsp;&nbsp;&nbsp;- 기능: 음소 간 차이를 바탕으로 오류 위치 및 맥락 정보 구조화<br>&nbsp;&nbsp;&nbsp;&nbsp;- 구현: `diff_by_type()` 함수에서 다음 구조를 생성:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• `diff`: 잘못 발음한 음소와 정답 음소 쌍<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• `pronunciationErrors`: 틀린 글자의 위치, 잘못된 음소, 올바른 음소<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• `errorAnalysis`: 초/중/종성 유형, 앞뒤 맥락 등 분석<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;음소-글자 간 매핑을 통해 오류의 위치를 정밀하게 추적<br><br><br>**2. 음성합성(TTS)을 통해 정확한 발음 제공**<br>정확한 한국어 발음 예시를 제공하기 위해 Google Cloud Text-to-Speech API를 도입하고, 이를 활용하여 음성 생성 및 응답 처리를 구성하였다.<br><br>&nbsp;&nbsp;동작 흐름:<br>&nbsp;&nbsp;&nbsp;&nbsp;a. 클라이언트가 학습하고자 하는 문장 또는 단어를 선택<br>&nbsp;&nbsp;&nbsp;&nbsp;b. 선택된 텍스트가 백엔드 서버에 전달되며, TTS 요청이 발생<br>&nbsp;&nbsp;&nbsp;&nbsp;c. 백엔드 서버는 해당 텍스트를 AI 서버로 전달<br>&nbsp;&nbsp;&nbsp;&nbsp;d. AI 서버는 Google TTS API를 호출하여 텍스트를 음성(`byte[]`)으로 변환해 백엔드에 반환<br>&nbsp;&nbsp;&nbsp;&nbsp;e. 백엔드는 수신한 음성 데이터를 AWS S3에 저장하고, 해당 S3 URL을 생성<br>&nbsp;&nbsp;&nbsp;&nbsp;f. 백엔드는 해당 URL을 클라이언트에 전달하고, 사용자는 음성을 재생하여 학습에 활용<br><br>&nbsp;&nbsp;● **Google TTS API (AI 서버 내 호출)**<br>&nbsp;&nbsp;&nbsp;&nbsp;- 기능: 입력된 텍스트를 자연스러운 한국어 음성으로 합성<br>&nbsp;&nbsp;&nbsp;&nbsp;- 구현: AI 서버에서 Google Text-to-Speech API를 사용하여 텍스트(`ko-KR`)를 음성으로 변환하고, `byte[]` 형태로 백엔드에 전달<br><br>&nbsp;&nbsp;● **음성 저장 모듈 (백엔드)**<br>&nbsp;&nbsp;&nbsp;&nbsp;- 기능: AI 서버로부터 받은 음성 데이터를 S3에 저장하고 URL 생성<br>&nbsp;&nbsp;&nbsp;&nbsp;- 구현: 백엔드는 `byte[]` 음성 데이터를 AWS S3 버킷에 업로드하고, 해당 파일의 공개 접근 가능한 URL 또는 Presigned URL을 생성하여 클라이언트에 응답<br><br>&nbsp;&nbsp;● **TTS 요청 중계 모듈 (백엔드)**<br>&nbsp;&nbsp;&nbsp;&nbsp;- 기능: 클라이언트 요청을 AI 서버로 전달하고, 응답 받은 음성 데이터를 저장 후 URL 전달<br>&nbsp;&nbsp;&nbsp;&nbsp;- 구현: `RestTemplate`을 사용해 AI 서버에 요청하고, 반환된 음성 데이터를 S3에 저장한 후 URL을 클라이언트에 응답<br><br><br>**3. GPT 기반 문장 교정 및 대화 연습**<br> &nbsp;해당 기능은 LangChain을 통해 OpenAI GPT-3.5 모델을 호출하며, 사용자 입력에 대해 발음 피드백과 문법 피드백, 자연스러운 대화 응답을 제공한다. <br><br> &nbsp; 동작 흐름:<br>  &nbsp;&nbsp; a. 토킹봇이 대화를 시작<br> &nbsp;&nbsp; b. 학습자 입력 음성 녹음 → 서버 전송<br>  &nbsp;&nbsp; c. 서버에서 Wav2Vec2 모델로 STT 변환 <br>  &nbsp;&nbsp; d. STT 결과 문장을 GPT-3.5로 전달하여 응답 생성<br> &nbsp;&nbsp;&nbsp; •	발음 교정 요청: get_pronunciation_feedback()	<br> &nbsp;&nbsp;&nbsp; •	문법 교정 요청: get_grammar_feedback()	<br>&nbsp;&nbsp;&nbsp; •	대화 요청: get_bot_reply()<br>  &nbsp;&nbsp; e. 생성된 JSON 응답을 파싱하여 사용자에게 전송<br>  &nbsp;&nbsp; f. 학습자는 응답 확인 및 재입력 반복<br>  &nbsp;&nbsp; g. 세션 종료 시, 피드백 로그를 DB에 저장<br><br>&nbsp;&nbsp;● **발음 평가 모듈 (AI 서버 내 호출)** <br>&nbsp;&nbsp; - 기능: 사용자의 발음에 대해 피드백 <br>&nbsp;&nbsp; - 구현: 위와 동일 <br><br>&nbsp;&nbsp;● **get_grammar_feedback() (AI 서버 내 호출)** <br>&nbsp;&nbsp; - 기능: 사용자의 문장을 분석해 문법 오류를 수정하고 그 이유를 설명<br>&nbsp;&nbsp;	- 구현: <br>&nbsp;&nbsp;&nbsp;	•	입력: 학습자의 문장 (sentence)	<br>&nbsp;&nbsp;&nbsp; • 출력: 피드백 JSON 객체 <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `{  "isFeedback": true,`<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;	 `   "suggestion": "나는 어제 도서관에 갔어요.",` <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `   "explanation": "You missed a postposition. '에' is needed to indicate direction."}`<br>	&nbsp;&nbsp;&nbsp; •	프롬프트 구성:	<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; •	문장이 맞는지 판단	<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; •	틀리면 수정안과 설명	<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; •	맞으면 isFeedback: false<br><br>&nbsp;&nbsp;● **get_bot_reply() (AI 서버 내 호출)** <br>&nbsp;&nbsp; - 기능: 사용자의 문장에 대해 친근한 한국어 응답을 생성하고 대화를 이어가기 위한 질문 포함 <br>&nbsp;&nbsp;	- 구현: <br>&nbsp;&nbsp;&nbsp;	•	입력: 학습자의 문장 (sentence)	<br>&nbsp;&nbsp;&nbsp; •	출력: 대화 응답 JSON 객체<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; `{  "korean": "안녕, 오늘 어땠어?",`<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;	 `   "translation": “Hi, how was your day today?”` <br>	&nbsp;&nbsp;&nbsp; •	프롬프트 구성:	<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; •	톤을 맞춰서 반말 or 존댓말	<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; •	follow-up 질문 포함	<br><br>&nbsp;&nbsp;● **중계 모듈 (백엔드)** <br>&nbsp;&nbsp;&nbsp;&nbsp;- 기능: 클라이언트로부터 받은 음성 데이터를 AI 서버로 전달하고, 받은 GPT 피드백 응답(JSON)을 파싱하여 DB에 저장 후 통합 응답으로 구성하여 클라이언트에 전달<br>&nbsp;&nbsp;&nbsp;&nbsp;- 구현:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• `/api/chat/feedback` 엔드포인트에 음성 파일 전송하면, 백엔드 서버는 AI 서버에 전달하여 받은 응답 제공<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;• `/api/chat/reply` 엔드포인트에 변환된 텍스트를 전송하면, 백엔드 서버는 AI 서버에 전달하여 받은 챗봇 응답과 TTS 요청을 하여 받은 음성 제공<br><br><br>**4. 개인 맞춤형 학습 경로 추천**<br>a. 학습자가 발음한 문장과 Whisper 분석 결과가 피드백 DB에 저장됨<br>b. 서버는 피드백 DB를 기반으로, 학습자의 발음 오류 패턴(음소, 단어, 문장 유형 등)을 분석함<br>c. 자주 오류가 발생한 음소나 발음 난이도 태그를 중심으로 훈련이 필요한 항목을 도출함<br>d. 해당 항목을 기반으로,<br>	•	(1) 기존 문장 DB에서 태그 기반 문장 추천 또는<br>	•	(2) GPT를 활용해 맞춤형 문장을 실시간 생성<br>하여 문장 리스트를 구성함<br>e. 필터링 또는 난이도 조정 로직을 통해 학습자의 레벨에 적합한 문장 리스트를 최종 확정함<br>f. 구성된 문장 리스트를 Client(학습자)에게 제공하여, 개인화된 발음 훈련을 유도함<br>g. 학습자가 연습을 거듭할수록 피드백 DB가 지속적으로 업데이트되며, 추천 문장도 동적으로 변화함|
| (5) 기타 |   |

<br>
